{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133168e7",
   "metadata": {},
   "source": [
    "AI Exploration — Evidence Notebook (PDF‑safe, two datasets, clean text tables)\n",
    "\n",
    "We have two piles of letters. One pile is perfect because we wrote each letter clearly and kept the boxes balanced. The other pile is real and messy. We will sort both piles into five boxes called delivery_issue, refund_request, billing_problem, app_bug, and other. We keep the view simple and make the tables PDF‑friendly by printing them as plain text boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f71f1",
   "metadata": {},
   "source": [
    "Finding the project root\n",
    "\n",
    "A notebook inside notebooks often points to the wrong place when we use relative paths. We walk up until we see a folder that has data and src. That folder is our root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(max_up=8):\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents][:max_up+1]:\n",
    "        if (p/'data').exists() and (p/'src').exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "ROOT = find_project_root()\n",
    "DATA_RAW = ROOT/'data'/'raw'\n",
    "MODELS = ROOT/'models'\n",
    "FIGS = ROOT/'reports'/'figures'\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('ROOT  :', ROOT)\n",
    "print('RAW   :', DATA_RAW)\n",
    "print('MODELS:', MODELS)\n",
    "print('FIGS  :', FIGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58ec44",
   "metadata": {},
   "source": [
    "Imports and small helpers\n",
    "\n",
    "We keep tables as simple text using the tabulate library so they render in PDF. If tabulate is missing, install it once with pip install tabulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcce8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "\n",
    "pd.set_option('display.max_colwidth', 160)\n",
    "\n",
    "def ellipsize(s, n=140):\n",
    "    s = str(s).replace('\\n',' ').strip()\n",
    "    return s if len(s) <= n else s[:n-1] + '…'\n",
    "\n",
    "def print_table(df, title=None):\n",
    "    if title:\n",
    "        print('\\n' + title)\n",
    "    print(tabulate(df, headers='keys', showindex=False, tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4228f7e",
   "metadata": {},
   "source": [
    "Loading the two piles\n",
    "\n",
    "We look for customer_feedback_1000.csv (perfect) and customer_feedback_sample.csv (real). Both must have text and category columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc94d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_if_exists(path: Path, tag: str):\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        if {'text','category'}.issubset(df.columns):\n",
    "            df = df.dropna(subset=['text','category']).drop_duplicates().reset_index(drop=True)\n",
    "            print(f'Loaded {tag}:', path, 'rows =', len(df))\n",
    "            return df\n",
    "        else:\n",
    "            print(f'Found {tag} at {path} but columns are {df.columns.tolist()} (need text, category)')\n",
    "    else:\n",
    "        print(f'{tag} not found at', path)\n",
    "    return None\n",
    "\n",
    "PERFECT_PATH = DATA_RAW/'customer_feedback_1000.csv'\n",
    "REAL_PATH    = DATA_RAW/'customer_feedback_sample.csv'\n",
    "\n",
    "df_perfect = load_csv_if_exists(PERFECT_PATH, 'perfect')\n",
    "df_real    = load_csv_if_exists(REAL_PATH, 'real')\n",
    "\n",
    "if df_perfect is None and df_real is None:\n",
    "    raise FileNotFoundError('Place customer_feedback_1000.csv or customer_feedback_sample.csv under data/raw with columns: text, category.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472c2a7",
   "metadata": {},
   "source": [
    "One run of the sorter\n",
    "\n",
    "We split with a fixed seed, build TF‑IDF features on single words and two‑word phrases, train Complement Naive Bayes, and then show compact plain‑text tables for the report and misclassifications. The confusion matrix is one clear plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def vectorizer_default():\n",
    "    return TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        ngram_range=(1,2),\n",
    "        min_df=5,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True,\n",
    "        lowercase=True\n",
    "    )\n",
    "\n",
    "def run_once(df, tag):\n",
    "    # Split\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, random_state=42, stratify=df['category']\n",
    "    )\n",
    "    # Vectorize\n",
    "    vec = vectorizer_default()\n",
    "    X_train = vec.fit_transform(train_df['text'])\n",
    "    y_train = train_df['category'].values\n",
    "    X_test  = vec.transform(test_df['text'])\n",
    "    y_test  = test_df['category'].values\n",
    "\n",
    "    # Train\n",
    "    clf = ComplementNB(alpha=0.3)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and metrics\n",
    "    pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    f1m = f1_score(y_test, pred, average='macro')\n",
    "    print(f'\\n{tag} accuracy:', round(acc,3), '| macro-F1:', round(f1m,3))\n",
    "\n",
    "    # Short classification report as text table\n",
    "    rep = classification_report(y_test, pred, output_dict=True, zero_division=0)\n",
    "    rep_df = (pd.DataFrame(rep).T\n",
    "                .rename(columns={'precision':'prec','recall':'rec','f1-score':'f1','support':'n'}))\n",
    "    order = [c for c in rep_df.index if c not in ('accuracy','macro avg','weighted avg')] + ['macro avg','weighted avg','accuracy']\n",
    "    rep_df = rep_df.loc[order][['n','prec','rec','f1']].round(3).reset_index().rename(columns={'index':'label'})\n",
    "    print_table(rep_df, title=f'{tag} classification report')\n",
    "\n",
    "    # Confusion matrix\n",
    "    labels = sorted(df['category'].unique())\n",
    "    cm = confusion_matrix(y_test, pred, labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.imshow(cm, aspect='auto')\n",
    "    ax.set_title(f'Confusion Matrix — {tag} (CNB)')\n",
    "    ax.set_xticks(range(len(labels))); ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right'); ax.set_yticklabels(labels)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, str(cm[i,j]), ha='center', va='center', fontsize=10)\n",
    "    plt.tight_layout(); plt.show()\n",
    "    fig.savefig(FIGS/f'confusion_matrix_{tag}.png', dpi=150)\n",
    "\n",
    "    # Misclassified examples: compact, plain text\n",
    "    proba = clf.predict_proba(X_test)\n",
    "    maxp  = proba.max(axis=1)\n",
    "    rows = []\n",
    "    for true, pr, text, p in zip(y_test, pred, test_df['text'].tolist(), maxp):\n",
    "        if true != pr:\n",
    "            t = text.lower()\n",
    "            if ('refund' in t or 'money back' in t or 'chargeback' in t) and ('charge' in t or 'fee' in t or 'statement' in t):\n",
    "                reason = 'refund vs billing overlap'\n",
    "            elif ('app' in t or 'mobile' in t or 'website' in t) and ('error' not in t and 'crash' not in t and 'login' not in t):\n",
    "                reason = 'vague app wording'\n",
    "            elif ('not received' in t or 'never received' in t or 'delivery' in t or 'arrived' in t) and ('tracking' not in t and 'delayed' not in t):\n",
    "                reason = 'weak delivery cues'\n",
    "            elif p < 0.55:\n",
    "                reason = 'low confidence'\n",
    "            else:\n",
    "                reason = 'mixed or ambiguous'\n",
    "            rows.append({'true': true, 'pred': pr, 'confidence': round(float(p),3), 'text': ellipsize(text), 'reason': reason})\n",
    "    if rows:\n",
    "        wrong_df = pd.DataFrame(rows).sort_values('confidence').head(12)\n",
    "        print_table(wrong_df, title=f'{tag} misclassified examples (12 lowest confidence)')\n",
    "        # Save full list for appendix use\n",
    "        pd.DataFrame(rows).to_csv(ROOT/'reports'/f'misclassified_{tag}.csv', index=False, encoding='utf-8')\n",
    "        print('Saved full misclassifications to', ROOT/'reports'/f'misclassified_{tag}.csv')\n",
    "    else:\n",
    "        print('No misclassifications to show.')\n",
    "\n",
    "    # Top features per class: 8 per class, text table\n",
    "    feature_names = np.array(vec.get_feature_names_out())\n",
    "    logp = clf.feature_log_prob_\n",
    "    rows = []\n",
    "    for i, cls in enumerate(clf.classes_):\n",
    "        others = np.delete(logp, i, axis=0)\n",
    "        scores = logp[i] - others.mean(axis=0)\n",
    "        top_idx = np.argsort(scores)[::-1][:8]\n",
    "        for rank, j in enumerate(top_idx, 1):\n",
    "            rows.append({'class': cls, 'rank': rank, 'feature': feature_names[j]})\n",
    "    feats_df = pd.DataFrame(rows)\n",
    "    print_table(feats_df, title=f'{tag} top informative words (8 per class)')\n",
    "\n",
    "    # Save artifacts with tag\n",
    "    import joblib\n",
    "    joblib.dump(vec, MODELS/f'vectorizer_{tag}.joblib')\n",
    "    joblib.dump(clf, MODELS/f'classifier_cnb_{tag}.joblib')\n",
    "\n",
    "    return {'tag': tag, 'accuracy': acc, 'macro_f1': f1m}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e789cc8",
   "metadata": {},
   "source": [
    "Run both piles and compare in plain words\n",
    "\n",
    "We run the perfect pile if present, then the real pile if present. If both are present, we write a short comparison in text so the story is readable in PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "if df_perfect is not None:\n",
    "    summaries.append(run_once(df_perfect, 'perfect'))\n",
    "if df_real is not None:\n",
    "    summaries.append(run_once(df_real, 'real'))\n",
    "\n",
    "if len(summaries) == 2:\n",
    "    a, b = summaries\n",
    "    print('\\nComparison in words')\n",
    "    print(f'Perfect set: accuracy {a[\"accuracy\"]:.3f}, macro-F1 {a[\"macro_f1\"]:.3f}.')\n",
    "    print(f'Real set   : accuracy {b[\"accuracy\"]:.3f}, macro-F1 {b[\"macro_f1\"]:.3f}.')\n",
    "    print(f'Delta (real - perfect): accuracy {(b[\"accuracy\"]-a[\"accuracy\"]):.3f}, macro-F1 {(b[\"macro_f1\"]-a[\"macro_f1\"]):.3f}.')\n",
    "    print('The perfect set scores higher because the labels are clean and the messages are balanced.')\n",
    "    print('The real set drops because refund and billing words overlap, delivery is often vague, and other is a mixed drawer.')\n",
    "    print('Clean labels raise scores more than clever tricks. Keep a small hand‑labeled gold set to steer improvements.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cccc6",
   "metadata": {},
   "source": [
    "Closing thought\n",
    "\n",
    "The student is the model and the teacher is the label. When the teacher is clear the student learns faster. Clean labels and tight boxes move the scores more than tweaks. Change one thing at a time and keep the seed fixed so the story stays honest on paper.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}